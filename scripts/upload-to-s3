#!/usr/bin/env python
# Upload to S3

import argparse
import boto.s3.connection
import os
import sys
import urllib2
import yaml

def get_credentials_from_file(filename):
    with open(filename) as credentials:
        for line in credentials:
            k, _, v = line.partition('=')
            k = k.strip()

            if k == 'AWSAccessKeyId':
                key = v.strip()
            elif k == 'AWSSecretKey':
                secret = v.strip()
    return (key, secret, None)

def get_instance_credentials():
    url = 'http://169.254.169.254/latest/meta-data/iam/security-credentials'
    try:
        role = urllib2.urlopen(url).read()
        cred = urllib2.urlopen(url + '/' + role).read()
    except urllib2.URLError:
        return None

    cred = yaml.safe_load(cred)
    return (cred['AccessKeyId'], cred['SecretAccessKey'], cred['Token'])

def get_credentials_from_environment():
    if 'AWS_ACCESS_KEY' in os.environ \
            and 'AWS_SECRET_KEY' in os.environ:
        return (os.environ.get('AWS_ACCESS_KEY'),
                os.environ.get('AWS_SECRET_KEY'), None)

    if 'AWS_CREDENTIAL_FILE' in os.environ:
        return get_credentials_from_file(os.environ.get('AWS_CREDENTIAL_FILE'))

    return None

def get_credentials():
    key, secret, token = get_credentials_from_environment() \
            or get_instance_credentials()

    return {
        'aws_access_key_id': key,
        'aws_secret_access_key': secret,
        'security_token': token
    }

def get_s3_connection():
    return boto.s3.connection.S3Connection(**get_credentials())

def get_destination(source_file, source_strip, destination_root):
    destination_suffix = source_file[len(source_strip):]
    return destination_root + destination_suffix

def upload_files(source_prefix, sources, source_strip, destination_bucket, destination_root, recursive):
    for source in sources:
        source_path = os.path.join(source_prefix, source)
        if recursive and os.path.isdir(source_path):
            files = []
            try:
                files = os.listdir(source_path)
            except OSError, err:
                print "Error: Failed to access file '%s'" % err.filename
            else:
                upload_files(source_path, files, source_strip,
                             destination_bucket, destination_root,
                             recursive)
        else:
            destination_file = get_destination(source_path, source_strip, destination_root)
            destination_bucket.new_key(destination_file).set_contents_from_file(source_path)

parser = argparse.ArgumentParser(description='Upload files to S3, under a common prefix.')

parser.add_argument('--strip', default='', metavar='<prefix>',
        help='Common prefix to drop from source files. This requires that all sources start with the prefix.')
parser.add_argument('--recursive', action="store_true",
        help='If the source is a directory recursively copy its content')
parser.add_argument('source', nargs='+', help='Files to upload')
parser.add_argument('destination',
        help='Destination and common prefix for all uploaded files.')

config = parser.parse_args()

for src in config.source:
    if not src.startswith(config.strip):
        raise Exception('File missing common prefix: %s (%s)'
                % (src, config.strip))

if not config.destination.startswith('s3://'):
    raise Exception('Unsupported destination URL: %s' % config.destination)

bucket_name, _, destination_root = config.destination[5:].partition('/')

s3 = get_s3_connection()
bucket = s3.get_bucket(bucket_name, validate=False)
upload_files('', config.source, config.strip, bucket, destination_root, config.recursive)

